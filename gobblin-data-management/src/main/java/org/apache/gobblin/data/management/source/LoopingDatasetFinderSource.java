/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.gobblin.data.management.source;

import java.io.IOException;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Optional;
import java.util.Set;
import java.util.Spliterator;
import java.util.stream.Stream;
import java.util.stream.StreamSupport;

import org.apache.gobblin.configuration.ConfigurationKeys;
import org.apache.gobblin.configuration.SourceState;
import org.apache.gobblin.configuration.State;
import org.apache.gobblin.configuration.WorkUnitState;
import org.apache.gobblin.dataset.Dataset;
import org.apache.gobblin.dataset.IterableDatasetFinder;
import org.apache.gobblin.dataset.PartitionableDataset;
import org.apache.gobblin.dataset.URNIdentified;
import org.apache.gobblin.dataset.comparators.URNLexicographicalComparator;
import org.apache.gobblin.runtime.task.NoopTask;
import org.apache.gobblin.runtime.task.TaskFactory;
import org.apache.gobblin.runtime.task.TaskUtils;
import org.apache.gobblin.source.workunit.BasicWorkUnitStream;
import org.apache.gobblin.source.workunit.WorkUnit;
import org.apache.gobblin.source.workunit.WorkUnitStream;

import com.google.common.collect.AbstractIterator;
import com.google.common.collect.Iterators;
import com.google.common.collect.Lists;
import com.google.common.collect.PeekingIterator;

import javax.annotation.Nullable;
import lombok.extern.slf4j.Slf4j;


/**
 * A source that processes datasets generated by a {@link org.apache.gobblin.dataset.DatasetsFinder}, processing a few
 * of them each run, and continuing from where it left off in the next run. When it is done processing all the datasets,
 * it starts over from the beginning. The datasets are processed in lexicographical order based on URN.
 */
@Slf4j
public abstract class LoopingDatasetFinderSource<S, D> extends DatasetFinderSource<S, D> {

  private static final String LOOPING_SOURCE_CONFIG_KEY_PREFIX = "gobblin.source.loopingDatasetFinderSource.";
  public static final String MAX_WORK_UNITS_PER_RUN_KEY = LOOPING_SOURCE_CONFIG_KEY_PREFIX + "maxWorkUnitsPerRun";
  public static final int MAX_WORK_UNITS_PER_RUN = 10;

  public static final String DATASET_URN = LOOPING_SOURCE_CONFIG_KEY_PREFIX + "datasetUrn";
  private static final String PARTITION_URN = LOOPING_SOURCE_CONFIG_KEY_PREFIX + "partitionUrn";

  // WORK_UNIT_ORDINAL is used to identify the latest workunit processed in previous execution,
  // Since the order is Lexicographical when executing.
  private static final String WORK_UNIT_ORDINAL = LOOPING_SOURCE_CONFIG_KEY_PREFIX + "workUnitOrdinal";
  protected static final String END_OF_DATASETS_KEY = LOOPING_SOURCE_CONFIG_KEY_PREFIX + "endOfDatasets";
  public static final String IS_ROLLOVER_WU = LOOPING_SOURCE_CONFIG_KEY_PREFIX + "isRollOver";
  private static final String ROLLOVER_TASK_FACTORY_BACKUP =
      LOOPING_SOURCE_CONFIG_KEY_PREFIX + "rollOverTaskFactory.backup";

  private final URNLexicographicalComparator lexicographicalComparator = new URNLexicographicalComparator();

  /**
   * @param drilldownIntoPartitions if set to true, will process each partition of a {@link PartitionableDataset} as a
   *                                separate work unit.
   */
  public LoopingDatasetFinderSource(boolean drilldownIntoPartitions) {
    super(drilldownIntoPartitions);
  }

  @Override
  public List<WorkUnit> getWorkunits(SourceState state) {
    return Lists.newArrayList(getWorkunitStream(state).getMaterializedWorkUnitCollection());
  }

  /**
   * Collect all failed workunit in previous run into a list.
   * <p>
   * Workunits that exceed max retry limit will not be added into failedPreviousWorkUnits and rolled over into next
   * quota-available execution.
   */
  private List<WorkUnit> getPreviousFailedWorkUnits(List<WorkUnitState> previousWorkUnitStates, int maxRetries) {
    List<WorkUnit> failedPreviousWorkUnits = new ArrayList<>();
    for (WorkUnitState workUnitState : previousWorkUnitStates) {
      WorkUnitState.WorkingState workingState = workUnitState.getWorkingState();
      if (workingState == WorkUnitState.WorkingState.FAILED || isRollOverWU(workUnitState)) {
        int currentRetryCount = 0;
        if (workUnitState.contains(ConfigurationKeys.TASK_RETRIES_KEY)) {
          currentRetryCount = workUnitState.getPropAsInt(ConfigurationKeys.TASK_RETRIES_KEY);
        }
        if (currentRetryCount < maxRetries) {
          log.info("Dataset " + workUnitState.getProp(DATASET_URN) + " is failed previously, retrying...");
          // workUnitState.getWorkunit() return an ImmutableWorkUnit which doesn't support resetting the configuration,
          // since we need to reset retryCount it is necessary to re-instantiate it.
          WorkUnit retryWorkUnit = new WorkUnit(workUnitState.getWorkunit());
          failedPreviousWorkUnits.add(retryWorkUnit);
        } else {
          log.warn(
              "Dataset " + workUnitState.getProp(DATASET_URN) + " exceeds retry limit : " + maxRetries + ", skipped");
        }
      }
    }
    return failedPreviousWorkUnits;
  }

  /**
   * Track all dataset/partition's urns that is included in the retried workunits, so that in the case where the number
   * of remaining dataset/partitions are less than the maximum number of workunits can be processed by single execution,
   * a retried workunit won't be processed twice
   */
  private Set<String> populateUrnSetwithPreviousFailedWorkUnit(List<WorkUnit> failedPreviousWorkUnits) {
    Set<String> retriedUrns = new HashSet<>();
    for (WorkUnit workUnit : failedPreviousWorkUnits) {
      String retriedUrn =
          workUnit.contains(PARTITION_URN) ? workUnit.getProp(PARTITION_URN) : workUnit.getProp(DATASET_URN);
      log.info("The workunit: " + retriedUrn + " is added into retry bucket.");
      retriedUrns.add(retriedUrn);
    }
    return retriedUrns;
  }

  /**
   * Searching for workunit with maximum ordinal.
   * <p>
   * There's a case that all previous workunits are failedWorkUnits so that none of that contains WORK_UNIT_ORDINAL. For
   * this, we should avoid failedWorkUnits to be a candidate of the workunit with highest water that is to be processed
   * first in this loop.
   */
  private Optional<WorkUnitState> getMaxWorkUnitState(List<WorkUnitState> previousWorkUnitStates) {
    int maxWorkUnitOrdinalSeen = 0;
    Optional<WorkUnitState> optionalMaxWorkUnitState = Optional.empty();
    for (WorkUnitState workUnitState : previousWorkUnitStates) {
      if (!workUnitState.contains(WORK_UNIT_ORDINAL)) {
        continue;
      }
      try {
        if (workUnitState.getPropAsInt(WORK_UNIT_ORDINAL) > maxWorkUnitOrdinalSeen) {
          optionalMaxWorkUnitState = Optional.of(workUnitState);
          maxWorkUnitOrdinalSeen = workUnitState.getPropAsInt(WORK_UNIT_ORDINAL);
        }
      } catch (NumberFormatException nfe) {
        throw new RuntimeException("Work units in state store are corrupted! Missing or malformed " + WORK_UNIT_ORDINAL,
            nfe);
      }
    }
    return optionalMaxWorkUnitState;
  }

  @Override
  public WorkUnitStream getWorkunitStream(SourceState state) {
    try {
      int maxWorkUnits = state.getPropAsInt(MAX_WORK_UNITS_PER_RUN_KEY, MAX_WORK_UNITS_PER_RUN);

      List<WorkUnitState> previousWorkUnitStates = state.getPreviousWorkUnitStates();
      int maxRetries = state.contains(ConfigurationKeys.MAX_TASK_CROSSEXECUTION_TASK_RETRIES_KEY) ? state
          .getPropAsInt(ConfigurationKeys.MAX_TASK_CROSSEXECUTION_TASK_RETRIES_KEY)
          : ConfigurationKeys.DEFAULT_MAX_TASK_RETRIES;

      List<WorkUnit> failedPreviousWorkUnits = getPreviousFailedWorkUnits(previousWorkUnitStates, maxRetries);
      Set<String> retryUrns = populateUrnSetwithPreviousFailedWorkUnit(failedPreviousWorkUnits);
      Optional<WorkUnitState> optionalMaxWorkUnit = getMaxWorkUnitState(previousWorkUnitStates);

      String previousDatasetUrnWatermark = null;
      String previousPartitionUrnWatermark = null;
      if (optionalMaxWorkUnit.isPresent() && !optionalMaxWorkUnit.get().getPropAsBoolean(END_OF_DATASETS_KEY, false)) {
        previousDatasetUrnWatermark = optionalMaxWorkUnit.get().getProp(DATASET_URN);
        previousPartitionUrnWatermark = optionalMaxWorkUnit.get().getProp(PARTITION_URN);
      }

      IterableDatasetFinder datasetsFinder = createDatasetsFinder(state);
      Stream<Dataset> datasetStream =
          datasetsFinder.getDatasetsStream(Spliterator.SORTED, this.lexicographicalComparator);
      datasetStream = sortStreamLexicographically(datasetStream);

      return new BasicWorkUnitStream.Builder(
          new DeepIterator(datasetStream.iterator(), previousDatasetUrnWatermark, previousPartitionUrnWatermark,
              maxWorkUnits, failedPreviousWorkUnits.iterator(), retryUrns)).setFiniteStream(true).build();
    } catch (IOException ioe) {
      throw new RuntimeException(ioe);
    }
  }

  /**
   * A deep iterator that advances input streams until the correct position, then possibly iterates over partitions of
   * {@link PartitionableDataset}s.
   */
  private class DeepIterator extends AbstractIterator<WorkUnit> {
    private final Iterator<WorkUnit> retryWorkUnits;
    private final Set<String> retryUrns;
    private final Iterator<Dataset> baseIterator;
    private final int maxWorkUnits;

    private Iterator<PartitionableDataset.DatasetPartition> currentPartitionIterator;
    private int generatedWorkUnits = 0;

    /**
     * @param retryWorkUnits mainly designed for previously-failed workunit to be retried in new execution.
     */
    public DeepIterator(Iterator<Dataset> baseIterator, String previousDatasetUrnWatermark,
        String previousPartitionUrnWatermark, int maxWorkUnits, Iterator<WorkUnit> retryWorkUnits,
        Set<String> retryUrns) {
      this.maxWorkUnits = maxWorkUnits;
      this.baseIterator = baseIterator;
      this.retryWorkUnits = retryWorkUnits;
      this.retryUrns = retryUrns;

      Dataset equalDataset =
          advanceUntilLargerThan(Iterators.peekingIterator(this.baseIterator), previousDatasetUrnWatermark);

      if (drilldownIntoPartitions && equalDataset != null && equalDataset instanceof PartitionableDataset) {
        this.currentPartitionIterator = getPartitionIterator((PartitionableDataset) equalDataset);
        advanceUntilLargerThan(Iterators.peekingIterator(this.currentPartitionIterator), previousPartitionUrnWatermark);
      } else {
        this.currentPartitionIterator = Iterators.emptyIterator();
      }
    }

    /**
     * Advance an iterator until the next value is larger than the reference.
     *
     * @return the last value polled if it is equal to reference, or null otherwise.
     */
    @Nullable
    private <T extends URNIdentified> T advanceUntilLargerThan(PeekingIterator<T> it, String reference) {
      if (reference == null) {
        return null;
      }

      int comparisonResult = -1;
      while (it.hasNext() && (comparisonResult = lexicographicalComparator.compare(it.peek(), reference)) < 0) {
        it.next();
      }
      return comparisonResult == 0 ? it.next() : null;
    }

    private Iterator<PartitionableDataset.DatasetPartition> getPartitionIterator(PartitionableDataset dataset) {
      try {
        return this.currentPartitionIterator = sortStreamLexicographically(
            dataset.getPartitions(Spliterator.SORTED, LoopingDatasetFinderSource.this.lexicographicalComparator))
            .iterator();
      } catch (IOException ioe) {
        log.error("Failed to get partitions for dataset " + dataset.getUrn());
        return Iterators.emptyIterator();
      }
    }

    @Override
    protected WorkUnit computeNext() {
      if (this.generatedWorkUnits >= this.maxWorkUnits) {
        if (retryWorkUnits.hasNext()) {
          //For the case where retryWorkUnits.size() > maxWorkUnits, we need to keep track of retryWorkunits that aren't
          //able to fit in a single execution, by committing their state thereby being recognizable in next execution.

          // If a workunit has been rolled over previously, prepareRolloverWorkUnit method will do nothing.
          WorkUnit rollOverWorkUnit = retryWorkUnits.next();
          com.google.common.base.Optional<TaskFactory> taskFactory = TaskUtils.getTaskFactory(rollOverWorkUnit);
          prepareRolloverWorkUnit(rollOverWorkUnit,
              taskFactory.isPresent() ? taskFactory.get().getClass() : NoopTask.Factory.class);
          return rollOverWorkUnit;
        } else {
          return endOfData();
        }
      }

      if (retryWorkUnits.hasNext()) {
        WorkUnit workUnit = retryWorkUnits.next();
        int previousRetryCount = workUnit.contains(ConfigurationKeys.TASK_RETRIES_KEY) ?
            workUnit.getPropAsInt(ConfigurationKeys.TASK_RETRIES_KEY) + 1 : 1;
        workUnit.setProp(ConfigurationKeys.TASK_RETRIES_KEY, previousRetryCount);
        if (workUnit.getPropAsBoolean(IS_ROLLOVER_WU)) {
          resetRolloverWorkUnit(workUnit);
        }
        generatedWorkUnits++;
        return workUnit;
      }

      while (this.baseIterator.hasNext() || this.currentPartitionIterator.hasNext()) {
        if (this.currentPartitionIterator != null && this.currentPartitionIterator.hasNext()) {
          PartitionableDataset.DatasetPartition partition = this.currentPartitionIterator.next();
          // For partition appeared in retryUrns, just skip.
          if (retryUrns.contains(partition.getUrn())) {
            log.info("The partition:" + partition.getUrn()
                + " is skipped in baseIterator since it has been included in the retry phase");
            continue;
          }
          WorkUnit workUnit = workUnitForDatasetPartition(partition);
          addDatasetInfoToWorkUnit(workUnit, partition.getDataset(), this.generatedWorkUnits++);
          addPartitionInfoToWorkUnit(workUnit, partition);
          return workUnit;
        }

        Dataset dataset = this.baseIterator.next();
        // For dataset appeared in retryUrns, just skip.
        if (retryUrns.contains(dataset.getUrn())) {
          log.info("The dataset:" + dataset.getUrn()
              + " is skipped in baseIterator since it has been included in the retry phase");
          continue;
        }
        if (drilldownIntoPartitions && dataset instanceof PartitionableDataset) {
          this.currentPartitionIterator = getPartitionIterator((PartitionableDataset) dataset);
        } else {
          WorkUnit workUnit = workUnitForDataset(dataset);
          addDatasetInfoToWorkUnit(workUnit, dataset, this.generatedWorkUnits++);
          return workUnit;
        }
      }

      WorkUnit workUnit = NoopTask.noopWorkunit();
      workUnit.setProp(WORK_UNIT_ORDINAL, this.generatedWorkUnits);

      this.generatedWorkUnits = Integer.MAX_VALUE;

      workUnit.setProp(END_OF_DATASETS_KEY, true);
      return workUnit;
    }

    private void addDatasetInfoToWorkUnit(WorkUnit workUnit, Dataset dataset, int workUnitOrdinal) {
      workUnit.setProp(DATASET_URN, dataset.getUrn());
      workUnit.setProp(WORK_UNIT_ORDINAL, workUnitOrdinal);
    }

    private void addPartitionInfoToWorkUnit(WorkUnit workUnit, PartitionableDataset.DatasetPartition partition) {
      workUnit.setProp(PARTITION_URN, partition.getUrn());
    }
  }

  /**
   * Sort input stream lexicographically. Noop if the input stream is already sorted.
   */
  private <T extends URNIdentified> Stream<T> sortStreamLexicographically(Stream<T> inputStream) {
    Spliterator<T> spliterator = inputStream.spliterator();
    if (spliterator.hasCharacteristics(Spliterator.SORTED) && spliterator.getComparator()
        .equals(this.lexicographicalComparator)) {
      return StreamSupport.stream(spliterator, false);
    }
    return StreamSupport.stream(spliterator, false).sorted(this.lexicographicalComparator);
  }

  /**
   * Keep old {@link TaskFactory} class in workUnit while setting the exposing factory as {@link NoopTask.Factory}, so
   * that when the task is rolled over into next execution, the original taskFactory class can be recovered.
   * <p>
   * Note that rolling over a task is not a generic use case, but mostly specifc to LoopingDatasetFinderSource, so this
   * class is not put in {@link TaskUtils}.
   *
   * @param state
   * @param klazz
   */
  private void prepareRolloverWorkUnit(State state, Class<? extends TaskFactory> klazz) {
    // Since it is possible that a roll over task is rolled over several times, so this backup Task factory class
    // should only be set for once.
    // Do nothing for preparing an already-rollOver workunit.
    if (!isRollOverWU(state)) {
      state.setProp(ROLLOVER_TASK_FACTORY_BACKUP, klazz.getName());
      state.setProp(IS_ROLLOVER_WU, true);
      TaskUtils.setTaskFactoryClass(state, NoopTask.Factory.class);
    }
  }

  /**
   * Recover the RollOver workunit's original Factory class so that it could be executed as originally designed.
   *
   * @param state The ready-to-execute workunit.
   */
  private void resetRolloverWorkUnit(State state) {
    try {
      Class<? extends TaskFactory> clazz =
          Class.forName(state.getProp(ROLLOVER_TASK_FACTORY_BACKUP)).asSubclass(TaskFactory.class);
      TaskUtils.setTaskFactoryClass(state, clazz);
      state.removeProp(IS_ROLLOVER_WU);
    } catch (ReflectiveOperationException roe) {
      throw new RuntimeException("Could not create task factory.", roe);
    }
  }

  /**
   * Determine if a workunit is rolled over from previous execution or not.
   *
   * @param state The workUnit from previous execution or to be inspected.
   *
   * @return
   */
  private boolean isRollOverWU(State state) {
    return state.getPropAsBoolean(IS_ROLLOVER_WU);
  }
}
