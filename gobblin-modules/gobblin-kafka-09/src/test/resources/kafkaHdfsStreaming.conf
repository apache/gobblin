# A sample skeleton that reads from a Kafka topic and writes to Local FS in a streaming manner
# This sample job works with Embedded Gobblin using LocalJobLauncher instead of going through Yarn approach.

job.name=LocalKafkaStreaming
job.group=streaming
job.description=A getting started example for Gobblin streaming to Kafka
job.lock.enabled=false

# Flag to enable StreamModelTaskRunner
task.execution.synchronousExecutionModel=false
gobblin.task.is.single.branch.synchronous=true
taskexecutor.threadpool.size=1
fork.record.queue.capacity=1

# Streaming-source specific configurations
source.class=org.apache.gobblin.source.extractor.extract.kafka.UniversalKafkaSource
gobblin.source.kafka.extractorType=org.apache.gobblin.source.extractor.extract.kafka.KafkaStreamingExtractor
kafka.workunit.size.estimator.type=CUSTOM
kafka.workunit.size.estimator.customizedType=org.apache.gobblin.source.extractor.extract.kafka.workunit.packer.UnitKafkaWorkUnitSizeEstimator
kafka.workunit.packer.type=CUSTOM
kafka.workunit.packer.customizedType=org.apache.gobblin.source.extractor.extract.kafka.workunit.packer.KafkaTopicGroupingWorkUnitPacker
extract.namespace=org.apache.gobblin.streaming.test

# Configure watermark storage for streaming, using FS-based for local testing
streaming.watermarkStateStore.type=fc
streaming.watermark.commitIntervalMillis=2000

# Converter configs
# Default Generic Record based pipeline
# only use this config when kafka schemaRegistry is enabled
#recordStreamProcessor.classes="org.apache.gobblin.converter.GenericRecordBasedKafkaSchemaChangeInjector"
recordStreamProcessor.classes="org.apache.gobblin.converter.string.KafkaRecordToStringConverter, org.apache.gobblin.converter.string.StringToBytesConverter"
# Record-metadata decoration into main record
# This is not supported in OSS yet since we found decorate will require re-build generic record which is expansive
gobblin.kafka.converter.recordMetadata.enable=true

# Writer configs
writer.builder.class=org.apache.gobblin.writer.SimpleDataWriterBuilder
#writer.partitioner.class=org.apache.gobblin.writer.partitioner.TimeBasedAvroWriterPartitioner
writer.output.format=txt
#writer.partition.columns=header.time
#writer.partition.pattern=yyyy/MM/dd
writer.destination.type=HDFS
writer.staging.dir=/tmp/gobblin/streaming/writer-staging
writer.output.dir=/tmp/gobblin/streaming/writer-output
writer.closeOnFlush=true

state.store.enabled=false

# Publisher config
data.publisher.type=org.apache.gobblin.publisher.NoopPublisher
data.publisher.final.dir=/tmp/gobblin/kafka/publish
flush.data.publisher.class=org.apache.gobblin.publisher.TimePartitionedStreamingDataPublisher
###Config that controls intervals between flushes (and consequently, data publish)
stream.flush.interval.secs=60

### Following are Kafka Upstream related configurations
# Kafka source configurations
topic.whitelist=testEvents
bootstrap.with.offset=EARLIEST
source.kafka.fetchTimeoutMillis=3000
kafka.consumer.maxPollRecords=100

#Kafka broker/schema registry configs
#kafka.schema.registry.url=
#kafka.schema.registry.class=org.apache.gobblin.metrics.kafka.KafkaAvroSchemaRegistry
#kafka.schemaRegistry.class=org.apache.gobblin.metrics.kafka.KafkaAvroSchemaRegistry
#kafka.schemaRegistry.url=
kafka.brokers="localhost:9092"

#Kafka SSL configs
#security.protocol = SSL
#ssl.protocol = TLS
#ssl.trustmanager.algorithm =
#ssl.keymanager.algorithm =
#ssl.truststore.type =
#ssl.truststore.location =
#ssl.truststore.password =
#ssl.keystore.type =
#ssl.keystore.password =
#ssl.key.password =
#ssl.secure.random.implementation =
#ssl.keystore.location=<path to your kafka certs>

metrics.enabled=false

# consumer client config
gobblin.kafka.consumerClient.class="org.apache.gobblin.kafka.client.Kafka09ConsumerClient$Factory"
source.kafka.value.deserializer="org.apache.gobblin.source.extractor.extract.kafka.Kafka09JsonSource$KafkaGsonDeserializer"

# Only Required for Local-testing
kafka.consumer.runtimeIngestionPropsEnabled=false
# Limit single mappers for ease of debugging
mr.job.max.mappers = 1