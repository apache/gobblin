type=hadoopJava
job.class=org.apache.gobblin.azkaban.AzkabanGobblinYarnAppLauncher
job.name=GobblinKafkaStreaming
job.group=GobblinKafkaStreaming
job.lock.enabled=false
encrypt.key.loc=/jobs/kafkaetl/gobblin/master
cleanup.staging.data.per.task=false
user.to.proxy=kafkaetl

# This assumes all dependent jars are put into the 'lib' directory
job.jars=lib/*,resources/gobblin-site.xml
classpath=lib/*,/export/apps/hadoop/latest/lib/*,${hive.jars}

# MR Configurations
jvmArgsMem=-XX:MaxPermSize=128M
jvmArgsGc=-XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark
jvmArgsGcLog=-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -Xloggc:gc.log
jvmArgsPerf=-XX:+UseCompressedOops
jvmArgsError=-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/grid/a/mapred/tmp/
jvm.args=${jvmArgsMem} ${jvmArgsGc} ${jvmArgsGcLog} ${jvmArgsPerf} ${jvmArgsError}

mapreduce.output.fileoutputformat.compress=true
mapreduce.user.classpath.first=true
mapreduce.job.user.classpath.first=true

mapreduce.map.memory.mb=800
mapreduce.reduce.memory.mb=800
mapreduce.reduce.shuffle.input.buffer.percent=0.4
mapreduce.reduce.java.opts=${mapreduce.map.java.opts}
mapreduceMapJavaOptsMem=-Xmx4096m -Xms256m
mapreduceMapJavaOptsGc=-XX:+UseParNewGC
mapreduceMapJavaOptsGcLog=-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution
mapreduceMapJavaOptsPerf=-XX:+UseCompressedOops
mapreduceMapJavaOptsExtra=-Djava.net.preferIPv4Stack=true
java8.mem.metasizes=-XX:MaxMetaspaceSize=256m -XX:CompressedClassSpaceSize=256m -XX:ReservedCodeCacheSize=100M
mapreduce.map.java.opts=${mapreduceMapJavaOptsMem} ${mapreduceMapJavaOptsGc} ${mapreduceMapJavaOptsGcLog} ${mapreduceMapJavaOptsPerf} ${mapreduceMapJavaOptsExtra} ${java8.mem.metasizes}

# Common directories
gobblin.dataset.root.dir=${root.data.location}/tracking/streaming_parallel
# task.data.root.dir=${gobblin.dataset.root.dir}/_working/${root.project.name}/task-data
job.work.dir=/tmp/${root.project.name}/${job.name}
qualitychecker.row.err.file=${home.dir}/gobblin/${root.project.name}/err
job.lock.dir=${job.work.dir}/locks
state.store.dir=${home.dir}/gobblin/${root.project.name}/state-store
mr.job.root.dir=${home.dir}/gobblin/${root.project.name}/working
mr.job.lock.dir=${home.dir}/gobblin/${root.project.name}/locks
writer.staging.dir=${job.work.dir}/task-staging
writer.output.dir=${job.work.dir}/task-output

# Compaction specific directories
compaction.input.dir=${gobblin.dataset.root.dir}
compaction.dest.dir=${gobblin.dataset.root.dir}
compaction.input.subdir=hourly
compaction.dest.subdir=daily
compaction.tmp.dest.dir=/tmp/${root.project.name}/${job.name}

# FS URIs
source.filebased.fs.uri=${fs.uri}
writer.fs.uri=${fs.uri}
compaction.file.system.uri=${fs.uri}
state.store.fs.uri=${fs.uri}

# Helix Configuration
gobblin.cluster.helix.cluster.name=${gobblin.yarn.app.name}
gobblin.cluster.helix.overwrite=false
helix.job.timeout.seconds=9223372036854774
helix.task.timeout.seconds=9223372036854774

# Yarn Conffiguration
gobblin.yarn.app.name=${root.project.name}-${grid.name}
# No. of ms to wait between sending a SIGTERM and SIGKILL to a container
yarn.nodemanager.sleep-delay-before-sigkill.ms=30000
gobblin.yarn.work.dir=/tmp/${root.project.name}/${job.name}
gobblin.yarn.logs.sink.root.dir=${logs.dir}/gobblin/${root.project.name}/logs

# Gobblin Cluster
gobblin.cluster.specConsumer.path=${home.dir}/streaming/specs
gobblin.cluster.workflow.expirySeconds=9223372036854774
gobblin.cluster.job.conf.path=./jobs

# Audit Config
kafka.audit.schema.loc=/jobs/kafkaetl/gobblin/TrackingMonitoringEvent.avsc

# Topic specific configurations
kafka.topic.specific.state=[{"dataset" : "statfederatorevent", "writer.partition.columns":"auditHeader.time" }, {"dataset" :"jobactionmessage", "writer.partition.columns" : "auditHeader.time"}, {"dataset" : "hdfsauditevent", "writer.partition.columns" : "auditHeader.time"},{"dataset" : "mrjobsummaryevent", "writer.partition.columns" : "finishTime"},{"dataset" : "mrjobtaskevent", "writer.partition.columns" : "finishTime"},{"dataset" : "testrmauditingevent", "writer.partition.columns" : "auditHeader.time"},{"dataset" : "testmrcontainerevent", "writer.partition.columns" : "auditHeader.time"},{"dataset" : "testrmpreemptevent", "writer.partition.columns" : "auditHeader.time"},{"dataset" : "metastorepartitionauditevent", "writer.partition.columns" : "auditHeader.time"},{"dataset" : "metastoretableauditevent", "writer.partition.columns" : "auditHeader.time"},{"dataset" : "prestoquerycreatedeventv2", "writer.partition.columns" : "auditHeader.time" },{"dataset" : "prestoquerycompletedeventv2", "writer.partition.columns" : "auditHeader.time"},{"dataset" : "prestosplitcompletedeventv2", "writer.partition.columns" : "auditHeader.time"},{"dataset" : "rmcontainerevent", "writer.partition.columns" : "auditHeader.time"}]

#Kafka Consumer Client config
gobblin.kafka.consumerClient.class="com.linkedin.gobblinkafka.client.LiKafka10ConsumerClient$Factory"

#Kafka metadata fetcher config
kafkaMetaFetcher.abortWhenMisConfigureMetadata=false

# Hive registration common config
schema.literal.length.limit=2000000000
hive.partition.name=datepartition
hive.database.name=${hive.tracking.database.name}
hive.registration.enabled = true
hive.db.root.dir=/user/${user.name}
hive.table.partition.props=owner:${user.name}
hive.home=/export/apps/hive/latest/
hive.jars=${hive.home}/lib/*,${hive.home}/conf/,${hive.aux.jars.path}
hive.classpath.items=lib/*,./*,${hive.jars}
hive.aux.jars.path=/export/apps/hive/latest/aux/lib/*
hive.metastore.try.direct.sql=false
hive.upstream.data.attr.names=topic.name
hiveRegister.registerPartitionWithPullMode=true
hiveRegister.fetchLatestSchemaFromSchemaRegistry=true
# hive.row.format=org.apache.gobblin.hive.orc.HiveOrcSerDeManager

# Kafka SSL config
dynamicConfigGenerator.class=org.apache.gobblin.configuration.NoopDynamicConfigGenerator
azkaban.job.enable.ssl=false

#Topic Whitelisting/Blacklisting
orc.service.whitelist=MetricReport_Streaming,MetadataChangeEvent_v2,FailedMetadataChangeEvent_v2,HdfsAuditEvent,PrestoQueryCompletedEventV2,MetastorePartitionAuditEvent,MetastoreTableAuditEvent,MRJobSummaryEvent,MRJobTaskEvent
avro.whitelist=PageViewEvent,LoginEvent,UserRequestEvent,ScrapingScoreEvent,ScoreEvent,SecurityChallengeEvent,RegistrationEvent
orc.tracking.tier0.whitelist=PageViewEvent,LoginEvent,UserRequestEvent,ScrapingScoreEvent,ScoreEvent,SecurityChallengeEvent,RegistrationEvent,FederatedSearchEvent,FederatorResultEvent,MessageDeliveryEvent,InAppNotificationPersistedEvent,PlayerBeaconEvent,FeedServedEvent,FeedImpressionEvent,ControlInteractionEvent,RankingEvent,ProfileViewEvent,CommGenericMessageSendEvent,MooJobPostingsRankingEvent,LixTreatmentsEvent,NavigationEvent,MobileApplicationErrorEvent,AtcFeatureSnapshotEvent,TalentLandingPageViewEvent,VoyagerUserRequestEvent,EmailSendEvent,JobImpressionEventV2,SalesSearchServedEvent,MediaScannedEvent,CompanyStatusUpdateActivityEvent,PymkClientImpressionEvent,ViewportTrackingFeedServedEvent,NativeRealUserMonitoringEvent,RecommendationScoringEvent,BrowsemapImpressionEvent,FeedActionEvent,RealUserMonitoringEvent,ExternalAPIAccessEvent,AdExchangeBidResponseEvent,ConcourseFeatureSnapshotEvent,MeNotificationServedEvent,CapSearchRelevanceEvent,SalesCrmEntityResolutionEvent
orc.tracking.tier1.whitelist=AdImpressionEvent,AdClickEvent,VideoAdsActionEvent,AdLeadGenEvent,AdRealTimeBiddingClickEvent,AdRealTimeBiddingImpressionEvent,AdLeadEvent,AdRealTimeBiddingVideoActionEvent,AdLeadEvent,AdServingFunnelEvent,AdRequestEvent,AdAdvertiserActionEvent,CampaignServingFunnelEvent
#Set of topics that are not partitioned adequately or for which we have data volume skew across partitions. Pull
# these topics to a separate tier so that we can bin pack fewer partitions per container for such topics.
# In the future, our bin packing should be able to automatically bin pack based on topic volumes. For now, we have to
# statically manage such topics.
orc.tracking.tier2.whitelist=AdOptimizationEvent,AdAuctionEvent

#Topic Blacklist
#Temporarily blacklist these topics with recursive schemas.
common.blacklist=FederatorResultEvent,RankingEvent

daily.compaction.orc.whitelist=${orc.tracking.tier0.whitelist},${orc.tracking.tier1.whitelist},${orc.tracking.tier2.whitelist}
daily.compaction.orc.service.whitelist=${orc.service.whitelist}
daily.compaction.avro.whitelist=${avro.whitelist}

#Metrics reporting configurations
metrics.reporting.kafkaPusherClass=org.apache.gobblin.metrics.kafka.KafkaProducerPusher
metrics.reporting.kafka.topic.events=GobblinTrackingEvent_Streaming
metrics.reporting.kafka.topic.metrics=MetricReport_Streaming
metrics.reporting.kafka.enabled=true
metrics.reporting.kafka.avro.use.schema.registry=true
metrics.reporting.kafka.format=avro
metrics.report.interval=300000
metrics.reporting.custom.builders=com.linkedin.gobblinjobmetrics.MetricReportUtf8ReporterFactory,com.linkedin.gobblin.metrics.InGraphsReporterFactory

#Enable emission of container health metrics
gobblin.cluster.container.health.metrics.service.enabled=true

#Ingraphs related Configuration
inGraphsReporter.enabled=true
inGraphsReporter.amfServerUrl=${amfServer.url}
inGraphsReporter.appName=streaming
inGraphsReporter.shouldCustomizeHostName=true
inGraphsReporter.hostNamePrefix=gobblin-kafka-streaming
inGraphsReporter.hostNameSuffix=grid.linkedin.com
inGraphsReporter.configsToIncludeInHostName=grid.name,container.num
inGraphsReporter.metricsWhitelistPatterns=consumer-fetch-manager-metrics,consumer-metrics,container.health.metrics
#inGraphsReporter.metricsPrefix=consumer-fetch-manager-metrics,consumer-coordinator-metrics,consumer-metrics,consumer-node-metrics
#inGraphsReporter.metricsBlacklistPatterns="Event"
