# A sample skeleton that reads from a Kafka topic and writes to Local FS in a streaming manner
# This sample job works with Embedded Gobblin using LocalJobLauncher instead of going through Yarn approach.

job.name=LocalKafkaStreaming
job.group=streaming
job.description=A getting started example for Gobblin streaming to Kafka
job.lock.enabled=false

# Flag to enable StreamModelTaskRunner
task.execution.synchronousExecutionModel=false
gobblin.task.is.single.branch.synchronous=true
taskexecutor.threadpool.size=1
fork.record.queue.capacity=1

# Streaming-source specific configurations
source.class=org.apache.gobblin.source.extractor.extract.kafka.UniversalKafkaSource
gobblin.source.kafka.extractorType=org.apache.gobblin.source.extractor.extract.kafka.KafkaStreamingExtractor
kafka.workunit.size.estimator.type=CUSTOM
kafka.workunit.size.estimator.customizedType=org.apache.gobblin.source.extractor.extract.kafka.workunit.packer.UnitKafkaWorkUnitSizeEstimator
kafka.workunit.packer.type=CUSTOM
kafka.workunit.packer.customizedType=org.apache.gobblin.source.extractor.extract.kafka.workunit.packer.KafkaTopicGroupingWorkUnitPacker
extract.namespace=org.apache.gobblin.streaming.test

# Configure watermark storage for streaming, using FS-based for local testing
streaming.watermarkStateStore.type=fs
streaming.watermark.commitIntervalMillis=2000

# Converter configs
# Default Generic Record based pipeline
recordStreamProcessor.classes="org.apache.gobblin.converter.GenericRecordBasedKafkaSchemaChangeInjector"

# Record-metadata decoration into main record
# This is not supported in OSS yet since we found decorate will require re-build generic record which is expansive
gobblin.kafka.converter.recordMetadata.enable=true

# Writer configs
writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder
writer.partitioner.class=org.apache.gobblin.writer.partitioner.TimeBasedAvroWriterPartitioner
writer.output.format=AVRO
writer.partition.columns=header.time
writer.partition.pattern=yyyy/MM/dd
writer.destination.type=HDFS
writer.staging.dir=/tmp/gobblin/streaming/writer-staging
writer.output.dir=/tmp/gobblin/streaming/writer-output
writer.closeOnFlush=true

state.store.enabled=false

# Publisher config
data.publisher.type=org.apache.gobblin.publisher.NoopPublisher
data.publisher.final.dir=/tmp/gobblin/kafka/publish
flush.data.publisher.class=org.apache.gobblin.prototype.kafka.TimePartitionedStreamingDataPublisher
###Config that controls intervals between flushes (and consequently, data publish)
stream.flush.interval.secs=60

### Following are Kafka Upstream related configurations
# Kafka source configurations
topic.whitelist=
bootstrap.with.offset=EARLIEST
source.kafka.fetchTimeoutMillis=3000
kafka.consumer.maxPollRecords=100

#Kafka broker/schema registry configs
kafka.schema.registry.url=
kafka.schema.registry.class=
kafka.schemaRegistry.class=
kafka.schemaRegistry.url=
kafka.brokers=

#Kafka SSL configs
security.protocol = SSL
ssl.protocol = TLS
ssl.trustmanager.algorithm =
ssl.keymanager.algorithm =
ssl.truststore.type =
ssl.truststore.location =
ssl.truststore.password =
ssl.keystore.type =
ssl.keystore.password =
ssl.key.password =
ssl.secure.random.implementation =
ssl.keystore.location=<path to your kafka certs>

metrics.enabled=false

# Only Required for Local-testing
kafka.consumer.runtimeIngestionPropsEnabled=false
# Limit single mappers for ease of debugging
mr.job.max.mappers = 1