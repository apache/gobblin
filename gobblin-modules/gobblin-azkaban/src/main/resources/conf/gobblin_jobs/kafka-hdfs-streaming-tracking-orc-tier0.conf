# A sample job that reads from a Kafka topic in EI and writes to Local FS in a streaming manner
# This sample job works with Embedded Gobblin using LocalJobLauncher instead of going through Yarn approach.

job.name=LocalKafkaStreaming
job.group=streaming
job.description=A getting started example for Gobblin streaming to Kafka
job.lock.enabled=false

# Flag to enable StreamModelTaskRunner
task.execution.synchronousExecutionModel=false
gobblin.task.is.single.branch.synchronous=true
taskexecutor.threadpool.size=1
fork.record.queue.capacity=1

# Streaming-source specific configurations
source.class=org.apache.gobblin.source.extractor.extract.kafka.UniversalKafkaSource
gobblin.source.kafka.extractorType=org.apache.gobblin.prototype.kafka.KafkaAvroBinaryStreamingExtractor
kafka.workunit.size.estimator.type=CUSTOM
kafka.workunit.size.estimator.customizedType=org.apache.gobblin.source.extractor.extract.kafka.workunit.packer.UnitKafkaWorkUnitSizeEstimator
kafka.workunit.packer.type=CUSTOM
kafka.workunit.packer.customizedType=org.apache.gobblin.source.extractor.extract.kafka.workunit.packer.KafkaTopicGroupingWorkUnitPacker
extract.namespace=org.apache.gobblin.streaming.test

# Configure watermark storage for streaming, using FS-based for local testing
streaming.watermarkStateStore.type=fs
streaming.watermark.commitIntervalMillis=2000

# Converter configs
# Default Generic Record based pipeline
recordStreamProcessor.classes="org.apache.gobblin.prototype.kafka.GenericRecordBasedKafkaSchemaChangeInjector,org.apache.gobblin.prototype.kafka.LiKafkaConsumerRecordToGenericRecordConverter"

# Record-metadata decoration into main record
gobblin.kafka.converter.recordMetadata.enable=true

# Writer configs
writer.builder.class=org.apache.gobblin.writer.AvroDataWriterBuilder
writer.partitioner.class=org.apache.gobblin.writer.partitioner.TimeBasedAvroWriterPartitioner
writer.output.format=AVRO
writer.partition.columns=header.time
writer.partition.pattern=yyyy/MM/dd
writer.destination.type=HDFS
writer.staging.dir=/tmp/gobblin/streaming/writer-staging
writer.output.dir=/tmp/gobblin/streaming/writer-output
writer.closeOnFlush=true

state.store.enabled=false

# Publisher config
data.publisher.type=org.apache.gobblin.publisher.NoopPublisher
data.publisher.final.dir=/tmp/gobblin/kafka/publish
# [DIFF] Simplify in-container logic
flush.data.publisher.class=org.apache.gobblin.prototype.kafka.TimePartitionedStreamingDataPublisher
###Config that controls intervals between flushes (and consequently, data publish)
stream.flush.interval.secs=60

### Following are Kafka Upstream related configurations

# Kafka source configurations
# Verified using kafka-tool that it has data.
# [DIFF] To make sure there's data, use earliest offset.
topic.whitelist=PageViewEvent
bootstrap.with.offset=EARLIEST
source.kafka.fetchTimeoutMillis=3000
kafka.consumer.maxPollRecords=100

#Kafka broker/schema registry configs
kafka.schema.registry.url="http://ltx1-schemaregistry-vip-2.stg.linkedin.com:10252/schemaRegistry/schemas"
kafka.schema.registry.class=org.apache.gobblin.metrics.kafka.KafkaAvroSchemaRegistry
kafka.schemaRegistry.class=org.apache.gobblin.kafka.schemareg.LiKafkaSchemaRegistry
kafka.schemaRegistry.url="http://ltx1-schemaregistry-vip-2.stg.linkedin.com:10252/schemaRegistry/schemas"
kafka.brokers="ltx1-kafka-kafka-tracking-vip.stg.linkedin.com:16637"

#Kafka SSL configs
security.protocol = SSL
ssl.protocol = TLS
ssl.trustmanager.algorithm = SunX509
ssl.keymanager.algorithm = SunX509
ssl.truststore.type = JKS
ssl.truststore.location = /etc/riddler/cacerts
ssl.truststore.password = changeit
ssl.keystore.type = pkcs12
ssl.keystore.password = work_around_jdk-6879539
ssl.key.password = work_around_jdk-6879539
ssl.secure.random.implementation = SHA1PRNG

##Change this config
ssl.keystore.location=/Users/lesun/.kafka-tool/certs/identity.p12

metrics.enabled=false
metrics.reporting.file.enabled=false
metrics.log.dir=/tmp/kafkaStreamingTest/metrics
metrics.reporting.custom.builders=com.linkedin.gobblin.metrics.InGraphsReporterFactory
inGraphsReporter.enabled=true
inGraphsReporter.amfServerUrl="http://ltx1-amf.stg.linkedin.com:13131/api/v1"
inGraphsReporter.appName=streaming
inGraphsReporter.shouldCustomizeHostName=true
inGraphsReporter.hostNamePrefix="gobblin-kafka-streaming-test"
inGraphsReporter.hostNameSuffix="grid.linkedin.com"
inGraphsReporter.configsToIncludeInHostName="fabric.name,job.name"
inGraphsReporter.metricsWhitelistPatterns="consumer-fetch-manager-metrics,consumer-metrics"
#inGraphsReporter.metricsBlacklistPatterns=Event

##Kafka Audit configurations
kafka.audit.tier="streaming-test-kafka-hdfs-corp-agg-ltx1"
metadata.broker.list="localhost:9092"

# Only Required for Local-testing
# Turning off runtime-ingestion properties which is primarily for ORC-rollout.
kafka.consumer.runtimeIngestionPropsEnabled=false
# Limit single mappers for ease of debugging
mr.job.max.mappers = 1
gobblin.kafka.consumerClient.class="com.linkedin.gobblinkafka.client.TestTopicLiKafkaConsumerClient$Factory"