#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# ====================================================================
# Job configurations (can be changed)
# ====================================================================

# General job metadata
job.group=DistcpHive
job.name=EmbeddedDistcpHiveTracking
job.description=Embedded Hive-Distcp-ng toy job

// URI comes from hiveConf, don't have to fill anything here.
hive.metastore.uri=""
hive.db.root.dir=/tmp

# Source and target metastores
hive.dataset.hive.metastore.uri=${hive.metastore.uri}
hive.dataset.copy.target.metastore.uri=${hive.metastore.uri}
gobblin.dataset.profile.class=org.apache.gobblin.data.management.copy.hive.HiveDatasetFinder

# Database and tables copy
hive.dataset.whitelist=testdb.test_table
#hive.dataset.copy.target.database=target

# Conflicting table and partitions treatment
hive.dataset.existing.entity.conflict.policy=REPLACE_PARTITIONS

# What to do with the files when deregistering a partition
hive.dataset.copy.deregister.fileDeleteMethod=NO_DELETE

# Which files to copy when copying a partition:
# INPUT_FORMAT -> use input format to find files, RECURSIVE -> copy all files under partition location
hive.dataset.copy.location.listing.method=RECURSIVE
# Should skip hidden paths when copying
hive.dataset.copy.locations.listing.skipHiddenPaths=true

# Use registration time (Hive parameter) to determine whether a partition should be skipped
hive.dataset.copy.fast.partition.skip.predicate=org.apache.gobblin.data.management.copy.predicates.RegistrationTimeSkipPredicate

# Partition filter
hive.dataset.copy.partition.filter.generator=org.apache.gobblin.data.management.copy.hive.filter.LookbackPartitionFilterGenerator
hive.dataset.partition.filter.datetime.column=datepartition
hive.dataset.partition.filter.datetime.lookback=P7D
hive.dataset.partition.filter.datetime.format=YYYY-MM-dd-HH

# Preserve attributes
gobblin.copy.preserved.attributes=rgbp

# Simulate?
gobblin.copy.simulate=false

# Bin packing
# 250 MB
gobblin.copy.binPacking.maxSizePerBin=250000000

# Trash
gobblin.trash.location=/data/trash/tracking/_GOBBLIN_TRASH

# target location for copy
data.publisher.final.dir=${hive.dataset.copy.target.table.prefixReplacement}

# ====================================================================
# Distcp configurations
# ====================================================================

extract.namespace=org.apache.gobblin.copy
data.publisher.type=org.apache.gobblin.data.management.copy.publisher.CopyDataPublisher
source.class=org.apache.gobblin.data.management.copy.CopySource
writer.builder.class=org.apache.gobblin.data.management.copy.writer.FileAwareInputStreamDataWriterBuilder
converter.classes=org.apache.gobblin.converter.IdentityConverter

task.maxretries=0
workunit.retry.enabled=false
user.defined.staging.dir.flag=false

distcp.persist.dir=/tmp/distcp-persist-dir

cleanup.staging.data.per.task=false
gobblin.trash.skip.trash=true
state.store.enabled=false
job.commit.parallelize=true
