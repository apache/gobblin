# Template for dumping data from Kafka to Hadoop compatible file system.
# By default simply dumps the records with no decoding or transformations.
# Required attributes: topics (topics to pull), job.name (name of the job), output (where to put the dump files)

gobblin.template.required_attributes="topics,job.name,output"

job.group=GobblinKafka
job.description=Gobblin quick start job for Kafka
job.lock.enabled=true

kafka.brokers="localhost:9092"

source.class=gobblin.source.extractor.extract.kafka.UniversalKafkaSource
gobblin.source.kafka.extractorType=DESERIALIZER
kafka.deserializer.type=BYTE_ARRAY

extract.namespace=gobblin.extract.kafka

writer.builder.class=gobblin.writer.SimpleDataWriterBuilder
simple.writer.delimiter="\n"
simple.writer.prepend.size=false

writer.file.path.type=tablename
writer.destination.type=HDFS
writer.output.format=txt

data.publisher.type=gobblin.publisher.TimePartitionedDataPublisher

bootstrap.with.offset=latest

topic.whitelist=${topics}
data.publisher.final.dir=${output}
