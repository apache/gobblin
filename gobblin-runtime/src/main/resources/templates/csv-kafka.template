#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# ====================================================================
# Job configurations
# ====================================================================

# Required configuration constraints
gobblin.template.required_attributes="input.fs.uri,input.dir,job.work.dir,state.store.fs.uri,writer.fs.uri,csv.schema.json,csv.kafka.brokers,csv.kafka.topic"

# Job info
job.name=CsvKafka
job.lock.enabled=false

# Gobblin data storage configurations
state.store.dir=${job.work.dir}/state-store
task.data.root.dir=${job.work.dir}/task-data
writer.staging.dir=${job.work.dir}/task-staging
writer.output.dir=${job.work.dir}/task-output
data.publisher.final.dir=${job.work.dir}/job-output

# Gobblin MapReduce configurations
mr.job.root.dir=${job.work.dir}/working
mr.job.lock.dir=${job.work.dir}/locks

# Source
source.class=org.apache.gobblin.source.extractor.filebased.TextFileBasedSource
source.filebased.downloader.class=org.apache.gobblin.source.extractor.filebased.CsvFileDownloader
source.schema=${csv.schema.json}

## Skip header
source.skip.first.record=true
source.filebased.fs.uri=${input.fs.uri}
source.filebased.data.directory=${input.dir}
source.filebased.maxFilesPerRun=1
source.max.number.of.partitions=3

## Extract
extract.namespace=data
extract.table.name=csv
extract.table.type=SNAPSHOT_APPEND

# Task execution configurations
taskexecutor.threadpool.size=4
taskretry.threadpool.coresize=1
taskretry.threadpool.maxsize=1

# Converter
converter.classes=org.apache.gobblin.converter.csv.CsvToJsonConverterV2

# Writer
writer.destination.type=KAFKA
writer.output.format=json
writer.builder.class=org.apache.gobblin.kafka.writer.Kafka09JsonObjectWriterBuilder

writer.kafka.topic=${csv.kafka.topic}
writer.kafka.producerConfig.value.serializer=org.apache.kafka.common.serialization.StringSerializer
writer.kafka.producerConfig.bootstrap.servers=${csv.kafka.brokers}
writer.kafka.producerConfig.retries=3
writer.kafka.producerConfig.client.id=CsvKafka