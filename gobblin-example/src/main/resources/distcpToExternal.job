# ====================================================================
# Job configurations (can be changed)
# ====================================================================

job.name=GobblinDatabaseCopyTest
job.description=Gobblin job for copy to S3



# target publishing location for copy
data.publisher.final.dir=/

gobblin.dataset.profile.class=gobblin.data.management.copy.CopyableGlobDatasetFinder
gobblin.dataset.pattern=/Users/lesun/Documents/localGobblinTest/localtest/*

# Necessary configuration to put for S3, for example
# For s3 to work, Need to also add hadoop-aws.jar as the dependency in the classpath. 
fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
fs.s3a.access.key=<DO NOT UPLOAD CREDENTIAL>
fs.s3a.secret.key=<DO NOT UPLOAD CREDENTIAL>
fs.s3a.buffer.dir=/tmp/buffer/
writer.fs.uri=s3a://gobblinoutput/testfolder

#You can disable ssl if necessary
#fs.s3a.connection.ssl.enabled=false


# ====================================================================
# Distcp configurations (do not change)
# ====================================================================

type=hadoopJava
job.class=gobblin.azkaban.AzkabanJobLauncher

extract.namespace=gobblin.copy

source.class=gobblin.data.management.copy.CopySource
converter.classes=gobblin.converter.IdentityConverter
writer.builder.class=gobblin.data.management.copy.writer.FileAwareInputStreamDataWriterBuilder
data.publisher.type=gobblin.data.management.copy.publisher.CopyDataPublisher
distcp.persist.dir=/tmp/distcp-persist-dir

task.maxretries=0
workunit.retry.enabled=false

# Intermediate steps configuration.
work.dir=/tmp/
state.store.dir=${work.dir}/state-store
writer.staging.dir=${work.dir}/taskStaging
writer.output.dir=${work.dir}/taskOutput
mr.job.root.dir=${work.dir}/working

